{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c32a32d-e36e-44c5-904e-8950a68b9a29",
   "metadata": {},
   "source": [
    "# Intro to Numba\n",
    "Numba is a Just-In-Time(JIT) compiler, optimal for high-computation Python. It can work for both CPU and GPU. \n",
    "\n",
    "Project Webpage: [https://numba.pydata.org/](https://numba.pydata.org/) \\\n",
    "Numba for GPU: [https://numba.readthedocs.io/en/stable/cuda/index.html](https://numba.readthedocs.io/en/stable/cuda/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6eb2e-076a-48df-8aab-8e5359da3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729ba45-74be-461f-a26a-6b627b595f6b",
   "metadata": {},
   "source": [
    "## Numba on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5270d9-5cc0-49e3-b243-14039402d14e",
   "metadata": {},
   "source": [
    "When using Numba on the GPU, using CuPy arrays is best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a9fb8-0392-40eb-8f06-f7506b4ba527",
   "metadata": {},
   "source": [
    "**We have two steps:**\n",
    "- **Kernel declaration**: \n",
    "  - A kernel function is a GPU function that is meant to be called from CPU code.\n",
    "  - Cannot explicitly return a value\n",
    "  - Is compiled once, it can be called multiple times with different block sizes or grid sizes)\n",
    "- **Kernel invocation:**\n",
    "  - Instantiate the kernel, by a **number of blocks** (or “blocks per grid”), and a **number of threads per block**\n",
    "  - Passing it the input array to the kernel and running it.\n",
    "  - Kernels run asynchronously. Use `cuda.synchronize()` to wait for all previous kernel launches to finish executing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd93ca8-a642-407e-b40a-43675eafed3e",
   "metadata": {},
   "source": [
    "Write just-in-time compiled kernels with Numba using the `@cuda.jit` decorator. \n",
    "\n",
    "Access thread position information using the following:\n",
    "* `cuda.grid(ndim)` returns absolute position of current thread in entire grid of blocks, given `ndim` is the number of dimensions the kernel was instantiated with.\n",
    "* `cuda.gridSize(ndim)` returns absolute size(shape) of the entire grid of blocks, in units of threads, given `ndim` is the number of dimensions the kernel was instantiated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a5a0c-12f2-452e-b20d-cd63991e09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def numba_double(x):\n",
    "    start = cuda.grid(1)     # absolute position of the current thread in the entire grid, here, ndim=1\n",
    "    step = cuda.gridsize(1)  # absolute size(shape) of the entire grid of blocks\n",
    "\n",
    "    for i in range(start, x.shape[0], step):\n",
    "        x[i] = 2*x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ad02c-66bf-480c-8b5b-62d2ac8746b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def numba_double_simple(x):\n",
    "    start = cuda.grid(1)     # absolute position of the current thread in the entire grid, here, ndim=1\n",
    "\n",
    "    for i in range(start, x.shape[0], 1):\n",
    "        x[i] = 2*x[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4111caa-1f00-42fd-8b4d-80c2960029c0",
   "metadata": {},
   "source": [
    "When we call the JIT kernel, we have to specify the `[blocks_per_grid, threads_per_block]`.\n",
    "\n",
    "Compare the performances of CPU NumPy, CPU Numba + NumPy, and GPU Numba + CuPy.\n",
    "\n",
    "Note: You may get a performance warning due to using a Numba CUDA kernel with Numpy. Since we're comparing performance, ignore the warning and rerun the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab97be8-a4aa-4979-ac03-028ffc675607",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [2**12, 2**13, 2**14, 2**15, 2**16, 2**17, 2**18, 2**19, 2**20, 2**21]\n",
    "numpy_times = []\n",
    "numba_numpy_times = []\n",
    "numba_cupy_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    x_cpu = np.random.randn(size)\n",
    "    x_gpu = cp.asarray(x_cpu)\n",
    "\n",
    "    threads_per_block = 32\n",
    "    blocks_per_grid = (x_cpu.size + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "    # CPU NumPy time\n",
    "    start = time.perf_counter()\n",
    "    output = x_cpu*2\n",
    "    numpy_times.append(time.perf_counter() - start)\n",
    "\n",
    "    # CPU Numba + NumPy time\n",
    "    start = time.perf_counter()\n",
    "    numba_double[blocks_per_grid, threads_per_block](x_cpu)\n",
    "    numba_numpy_times.append(time.perf_counter() - start)\n",
    "\n",
    "    # GPU Numba + CuPy time\n",
    "    start = time.perf_counter()\n",
    "    numba_double[blocks_per_grid, threads_per_block](x_gpu)\n",
    "    numba_cupy_times.append(time.perf_counter() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072bba6-3ac5-4147-8f47-c382c2a406c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Problem size\")\n",
    "plt.ylabel(\"Time to solve\")\n",
    "plt.plot(sizes, numpy_times, label=\"NumPy\")\n",
    "plt.plot(sizes, numba_numpy_times, label=\"Numba CPU\")\n",
    "plt.plot(sizes, numba_cupy_times, label=\"Numba GPU\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c1eb0-a6c9-4a11-8b72-383e9bdc8171",
   "metadata": {},
   "source": [
    "Here is an example of matrix multiplication, made faster by using shared memory.\n",
    "\n",
    "`cuda.threadIdx.x` gives the thread indices in the current thread block (in the range \\[0, `cuda.blockDim`\\])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a816f85-45a0-4297-b2f9-2d92a8754881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation will be done on blocks of threads_per_block x threads_per_block elements.\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    # Define arrays in the shared memory (size & type must be known at compile time)\n",
    "    sA = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float64)\n",
    "    sB = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float64)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    blocks_per_grid = cuda.gridDim.x\n",
    "\n",
    "    # Quit if (x, y) is outside of valid C boundary\n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of threads_per_block-length vectors.\n",
    "    tmp = float64(0.)\n",
    "    for i in range(blocks_per_grid):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * threads_per_block]\n",
    "        sB[tx, ty] = B[tx + i * threads_per_block, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(threads_per_block):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420df74-4b2c-4536-8bec-9e18a247a4c6",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe14cf3-36f7-4643-8650-c3a9951fed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8617b85-8105-41c1-9b96-1f45ac852bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.08-2",
   "language": "python",
   "name": "rapids-24.08-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
