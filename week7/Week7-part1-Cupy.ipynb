{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4941058a-6c14-4330-8dee-a9e137c516f3",
   "metadata": {},
   "source": [
    "# Intro to CuPy\n",
    "Project Webpage: [https://cupy.dev/](https://cupy.dev/) \\\n",
    "User Guide: [https://docs.cupy.dev/en/stable/user_guide/index.html](https://docs.cupy.dev/en/stable/user_guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f80bd-c004-4d16-9f1b-974c971d2235",
   "metadata": {},
   "source": [
    "Query information about the GPUs available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74837c5-d1ed-4a7f-ba8c-a0d99c668093",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfed62-bf03-4b04-b09f-d24cc9acc9c7",
   "metadata": {},
   "source": [
    "## CuPy vs NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1746304-2cd2-4242-8856-1ae4a5451cba",
   "metadata": {},
   "source": [
    "CuPy(CUDA Python) has very similar syntax to NumPy(Numerical Python).\n",
    "\n",
    "While NumPy arrays are stored on the CPU, CuPy arrays are stored on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1dd22-c226-4e76-a7a6-8fbdf5904521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "size = 2048\n",
    "\n",
    "# Initializes a random 2048x2048 matrix on the CPU\n",
    "A_cpu = np.random.rand(size, size).astype(np.float64)\n",
    "\n",
    "# Initializes a random 2048x2048 matrix on the GPU\n",
    "A_gpu = cp.random.rand(size, size).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef903d4-1b19-420e-9af7-6e01fbac0cf8",
   "metadata": {},
   "source": [
    "NumPy arrays can be changed into CuPy arrays by copying them from the CPU to the GPU, and vice versa. This conversion is not implicit, so you can't apply CuPy operations on NumPy arrays without copying them over first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b1352-4f5e-4be5-a425-07ec27f9a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array is initialized on the CPU\n",
    "B_cpu = np.random.randn(size, size)\n",
    "print(f\"B_cpu type: {type(B_cpu)}\")\n",
    "\n",
    "# Copy array from CPU(host) —> GPU(device)\n",
    "B_gpu = cp.asarray(B_cpu)\n",
    "print(f\"B_gpu type: {type(B_gpu)}\")\n",
    "\n",
    "# Apply calculations on the GPU\n",
    "B_gpu = cp.sin(B_gpu)\n",
    "\n",
    "# Copy array from GPU(device) —> CPU(host)\n",
    "B_cpu = cp.asnumpy(B_gpu)\n",
    "print(f\"B_cpu type: {type(B_cpu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c20c4-11f2-4d3b-90a2-1e8b72df2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot do:\n",
    "cp.sin(B_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a1170-8c07-45ec-89ac-6f7ee9c3cb56",
   "metadata": {},
   "source": [
    "### Let's plot a part of B_cpu and B_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d14301-0c22-499c-bccf-9cc0df6f5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.plot(B_cpu[0,:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204f7f2-23dc-4b14-b7b1-44ca79773941",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(B_gpu[0,:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96cef42-2a9d-406a-a0f6-bd1230e4710a",
   "metadata": {},
   "source": [
    "### let's compare the speed of operation\n",
    "\n",
    "Since operations on CuPy arrays are done on the GPU, they can be much faster than NumPy operations on the CPU, especially for dense linear algebra on large matrices.\n",
    "\n",
    "Note: `cp.cuda.Device().synchronize()` is used to ensure that the GPU operations are completed in order to time it accurately; it's not usually necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea20a5a-6269-48f9-8bab-cb12ded2a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy matrix multiplication\n",
    "%timeit -n 5 C_cpu = np.matmul(A_cpu, B_cpu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbf8de-0967-45db-a1af-ee3fed0dbe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CuPy matrix multiplication\n",
    "%timeit -n 5 C_gpu = cp.matmul(A_gpu, B_gpu); cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a5ab9-ced3-4f4b-a1f6-67412d184f3b",
   "metadata": {},
   "source": [
    "GPU can be worse? Re-run the above cells again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7c029-88e1-429f-b6b5-403d6ffaadb8",
   "metadata": {},
   "source": [
    "### Multiple GPUs\n",
    "CuPy also lets us work with data on multiple GPUs. Similar to the host/device, data has to be copied from one GPU to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ce6cc-e287-4c1d-afe7-d2e3f4da4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Will only work if you have more than 1 GPU)\n",
    "\n",
    "# Create array on GPU 1\n",
    "with cp.cuda.Device(1):\n",
    "    C_gpu1 = cp.zeros((size, size), dtype=cp.float)\n",
    "\n",
    "# Copy array from GPU 1 —> GPU 0\n",
    "with cp.cuda.Device(0): # not necessary, default device is 0\n",
    "    C_gpu0 = cp.asarray(C_gpu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920ff43-6ad6-48c6-b4ac-f1efd36584cd",
   "metadata": {},
   "source": [
    "## Overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45800018-2442-4ea6-950e-f4b808f3799b",
   "metadata": {},
   "source": [
    "There are 2 types of overhead to keep in mind when using the GPU with CuPy: **kernel overhead** and **data movement overhead**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c81baf-1585-4dbd-9e89-c364b2b27c46",
   "metadata": {},
   "source": [
    "### Kernel Overhead\n",
    "\n",
    "The first time a function is called in CuPy, there is compliation overhead because CuPy uses Just-In-Time(JIT) compilation. The next time the function is called again it uses the cached code, so it's not as slow.\n",
    "\n",
    "**The compiled codes are stored:** `~/.cupy/kernel_cache`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756329a0-8585-48d6-ac28-818cb2f4cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "\n",
    "for i in range(4):\n",
    "    D_gpu = cp.random.rand(size,size).astype(np.float64)\n",
    "    %time cp.linalg.eigh(D_gpu); cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871dffa-d1ae-4343-b5a8-5e8ad2238ba5",
   "metadata": {},
   "source": [
    "There is also a CUDA kernel launch overhead of a couple microseconds every time a new GPU kernel is launched. This overhead amortized by larger problem sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed562184-a700-4d3e-8dde-b161cf33aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [128, 256, 512, 1024]:\n",
    "    print(f\"\\nArray size {size}x{size}\")\n",
    "    \n",
    "    # NumPy\n",
    "    print(\"- NumPy time\")\n",
    "    E_cpu = np.random.rand(size,size).astype(np.float64)\n",
    "    %time np.linalg.eigh(E_cpu);\n",
    "\n",
    "    # CuPy\n",
    "    print(\"- CuPy time\")\n",
    "    E_gpu = cp.random.rand(size,size).astype(np.float64)\n",
    "    cp.linalg.eigh(E_gpu); #isolate out JIT compilation overhead\n",
    "    %time cp.linalg.eigh(E_gpu); cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41047d-983b-4138-bfee-e1c894884b99",
   "metadata": {},
   "source": [
    "The CUDA kernel launch overhead can also be reduced by merging multiple kernels together. We can see that by using the `@cupy.fuse` decorator, running the second fused kernel takes less time that the first kernel because it has no launch overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc73d2-8686-4439-98a8-72ffcd1dd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_multiply(x, y):\n",
    "    return 2*x*y\n",
    "\n",
    "@cp.fuse\n",
    "def double_multiply_fused(x,y):\n",
    "    return 2*x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7ccd0-a139-403c-83a9-02c1d069caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2**16\n",
    "F1 = cp.random.rand(size)\n",
    "F2 = cp.random.rand(size)\n",
    "\n",
    "double_multiply(F1, F2) #isolate out JIT compilation overhead\n",
    "%timeit -n 7 double_multiply(F1, F2); cp.cuda.Device().synchronize()\n",
    "\n",
    "double_multiply_fused(F1, F2) #isolate out JIT compilation overhead\n",
    "%timeit -n 7 double_multiply_fused(F1, F2); cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0813e-32c3-4d2f-aaad-5cf37f24c1ec",
   "metadata": {},
   "source": [
    "There is also an overhead associated when you run the very first CuPy function of a program, which is due to first creating the CUDA context by the CUDA driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3ab0a-cfc7-4970-a8ba-f0e986730e5b",
   "metadata": {},
   "source": [
    "### Data Movement Overhead\n",
    "\n",
    "Transferring data between the CPU and the GPU is slower than processing the data on the GPU, so minimizing data movement in or out of the GPU is best for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c519ae8-297c-4ab5-a453-db1b3fa39a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ded25a-3d8c-4132-bc17-fa8a5c9d4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data and operations on CPU\n",
    "\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start = time.perf_counter() #this function proviced high-resolution interval timing\n",
    "    \n",
    "    G_cpu = np.random.rand(size).astype(np.float64)\n",
    "    H_cpu = np.random.rand(size).astype(np.float64)\n",
    "    np.vdot(H_cpu, G_cpu);\n",
    "    \n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"All CPU takes on average {np.mean(times[-9:])*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be66dc4-e450-4ad9-86a8-1be7b0615740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data and operations on GPU\n",
    "\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    G_gpu = cp.random.rand(size).astype(np.float64)\n",
    "    H_gpu = cp.random.rand(size).astype(np.float64)\n",
    "    cp.vdot(H_gpu, G_gpu)\n",
    "    cp.cuda.Device().synchronize()\n",
    "    \n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"All GPU takes on average {np.mean(times[-9:])*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090673b4-a1b7-4873-b8f0-fae484353e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer data from CPU to GPU to operate on GPU\n",
    "\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    G_gpu = cp.asarray(G_cpu)\n",
    "    H_gpu = cp.asarray(H_cpu)\n",
    "    cp.vdot(H_gpu, G_gpu)\n",
    "    cp.cuda.Device().synchronize()\n",
    "\n",
    "    times.append(time.perf_counter() - start)\n",
    "\n",
    "print(f\"CPU —> GPU takes on average {np.mean(times[-9:])*1000} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90d561-7c4c-4cea-a128-426867a03d81",
   "metadata": {},
   "source": [
    "## GPU Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d190914-eb87-4451-ad6c-14983a0ece49",
   "metadata": {},
   "source": [
    "Query the free and total memory with `nvidia-smi` shell commands or in Python using CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f6ec6-c9fe-4239-8042-12bbfbd83b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b890a-9891-4e73-9cdd-16e220ca7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(memory free, memory total) in bytes:\")\n",
    "print(cp.cuda.Device().mem_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596cd4d-eb7b-4ded-847e-0ac830b3fd95",
   "metadata": {},
   "source": [
    "If you try to allocate too much memory on the GPU, you get an `OutOfMemory` error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9c59d-18ea-4860-9225-fdf3a3444733",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2**16\n",
    "I_gpu = cp.zeros((size, size))\n",
    "J_gpu = cp.zeros((size, size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50635dc2-e468-40f5-962a-c7c7b2e268ec",
   "metadata": {},
   "source": [
    "Clear all GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5108f9f-e88b-4f88-b2ae-506444b71657",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.get_default_memory_pool().free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c22d4-ad36-4b75-bafb-7d67a5fad21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -i 0 --query-gpu=memory.free,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36873b43-124e-4a4b-8851-bbce642c67e9",
   "metadata": {},
   "source": [
    "One way to resolve `OutOfMemory` errors is by using unified memory, where CUDA transfers data between the CPU and GPU on-demand (when page faults)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1ad79-19ad-491c-a3fb-778ec17be4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.cuda.set_allocator(cp.cuda.MemoryPool(cp.cuda.malloc_managed).malloc)\n",
    "\n",
    "size = 2**16\n",
    "I_gpu = cp.zeros((size, size))\n",
    "J_gpu = cp.zeros((size, size))\n",
    "# works when unified memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f984b7-3f9b-472a-9b88-afcd817746e0",
   "metadata": {},
   "source": [
    "Operations on these arrays can be slower due to the GPU moving pages in and out of its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac222f-4a8f-4eb1-991d-f048d4611daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cp.multiply(I_gpu, J_gpu)\n",
    "cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820b3f9-b750-4fd4-90f9-8e63f42231af",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbe283-6890-4758-b8b6-9d5f6d3fdfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceeb3ec-1102-49e9-9bd1-bb44986c5478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.08-2",
   "language": "python",
   "name": "rapids-24.08-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
